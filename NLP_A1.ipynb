{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyMh4WgpbXnQ7ld3VjQsmzt4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sinhajiya/NLP/blob/main/NLP_A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk import bigrams, trigrams\n",
        "import re"
      ],
      "metadata": {
        "id": "mhiE7ZkNuvmD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgT_t0_xZZJH",
        "outputId": "5d41d177-69cc-4f38-9f70-2bcc485afffb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1"
      ],
      "metadata": {
        "id": "uBEXeSZsuepk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cloning GitHub repo for dataset"
      ],
      "metadata": {
        "id": "jADAx3RPuVCU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-EISdf_t_FO",
        "outputId": "0bbef902-8f17-4e19-91ce-f576ed072477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Assignment_1_2025'...\n",
            "remote: Enumerating objects: 35, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 35 (delta 6), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (35/35), 1.06 MiB | 7.64 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/islnlp/Assignment_1_2025"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data and preprocessing"
      ],
      "metadata": {
        "id": "TYFOlUb2ub5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(name):\n",
        "  root_fp = f\"/content/Assignment_1_2025/{name}\"\n",
        "  train = pd.read_csv(os.path.join(root_fp, \"train.csv\"))\n",
        "  val = pd.read_csv(os.path.join(root_fp, \"val.csv\"))\n",
        "  train = train.dropna(subset=['Sentence'])\n",
        "  val = val.dropna(subset=['Sentence'])\n",
        "  return train, val"
      ],
      "metadata": {
        "id": "c816MxzhvWEN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(Sentence):\n",
        "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*(),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "    Sentence = Sentence.lower() # All lower case characters\n",
        "    Sentence = re.sub(url_pattern, \"\", Sentence)  # Remove all URLs\n",
        "    Sentence = re.sub(r\"\\.{2,}\", \".\", Sentence)  # Replace multiple dots with a single dot\n",
        "    Sentence = re.sub(r\"\\s+\", \" \", Sentence).strip()  # Remove extra spaces\n",
        "    return Sentence"
      ],
      "metadata": {
        "id": "ABazvC3PTjDs"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data(name):\n",
        "  train, val = load_data(name)\n",
        "  print(f\"Loading and preprocessing the {name} data... \\n\")\n",
        "  print(f\"Train shape: {train.shape}, Val shape: {val.shape}\\n\")\n",
        "  print(f\"Train head: \\n {train.head()}\\n\")\n",
        "  print(f\"Val head: \\n {val.head()}\\n\")\n",
        "  print(f\"The count of the tags in training data is \\n {train['Tag'].value_counts()}\\n\")\n",
        "  print(f\"The count of the tags in validation data is \\n {val['Tag'].value_counts()}\\n\")\n",
        "  train[\"Sentence_preprocessed\"] = train[\"Sentence\"].astype(str).apply(preprocess_text)\n",
        "  val[\"Sentence_preprocessed\"] = val[\"Sentence\"].astype(str).apply(preprocess_text)\n",
        "  print(f\"Before preprocessing: \\n\")\n",
        "  print(f\"{train['Sentence'][21]} \\n {train['Sentence'][27]}\")\n",
        "  print(f\"After preprocessing: \\n\")\n",
        "  print(f\"{train['Sentence_preprocessed'][21]} \\n {train['Sentence_preprocessed'][27]}\")\n",
        "\n",
        "  return train, val"
      ],
      "metadata": {
        "id": "Gx-AJd9kUMO0"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hate_train,hate_val = load_and_preprocess_data(\"hate\")"
      ],
      "metadata": {
        "id": "ImKUIH_qvRS1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7fb1f13-d0a5-4613-d0bd-49beee31db58"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing the hate data... \n",
            "\n",
            "Train shape: (3660, 2), Val shape: (457, 2)\n",
            "\n",
            "Train head: \n",
            "                                             Sentence  Tag\n",
            "0  #hariyana mey ek week mey teen bachchiyo ke Sa...    1\n",
            "1  indira Gandhi ko marne wala sikh.Rajiv Gandhi ...    1\n",
            "2  ishliye corruption ke jariye sab ki khoon choo...    1\n",
            "3  Pakistaniyon ko aisi news Maamul sa ho Gaya ha...    0\n",
            "4  Apne national anthem ko change karo and yeh li...    1\n",
            "\n",
            "Val head: \n",
            "                                             Sentence  Tag\n",
            "0          Sahee bolay ho kal Pakistani ka rape hoga    0\n",
            "1                    rape. Pyaar rape se kum nhi hai    0\n",
            "2  ye log mandir me hi q jakar rape krte kaya bi ...    0\n",
            "3  Delhi-Dehradun train mein 1seat par baithne k ...    0\n",
            "4  #GadhaAkallesh ne Kairana/Sahibabad/Buxar se H...    1\n",
            "\n",
            "The count of the tags in training data is \n",
            " Tag\n",
            "0    2307\n",
            "1    1353\n",
            "Name: count, dtype: int64\n",
            "\n",
            "The count of the tags in validation data is \n",
            " Tag\n",
            "0    309\n",
            "1    148\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Before preprocessing: \n",
            "\n",
            "Dohti walay terrorist nahi hotay  Sirf rape kertay hai \n",
            " This shows tolerance limit of Gujju .. Gujjuo ko badnaam din raat karta hai or Abhi tak ya zinda kaisa hai .. Kamaal ka thanda khoon hai gujjus ka https://twitter.com/sanjivbhatt/status/965166605541011456 …\n",
            "After preprocessing: \n",
            "\n",
            "dohti walay terrorist nahi hotay sirf rape kertay hai \n",
            " this shows tolerance limit of gujju . gujjuo ko badnaam din raat karta hai or abhi tak ya zinda kaisa hai . kamaal ka thanda khoon hai gujjus ka …\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction"
      ],
      "metadata": {
        "id": "baSqKgTXWRRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using N-gram language models (unigrams, bigrams, trigrams)\n",
        "as features."
      ],
      "metadata": {
        "id": "TkT3ldNxWaNS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fip30vfAUBLm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}